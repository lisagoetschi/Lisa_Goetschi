[
  {
    "objectID": "projects/bachelor_arbeit/index.html",
    "href": "projects/bachelor_arbeit/index.html",
    "title": "Bachelorarbeit und Poster",
    "section": "",
    "text": "Unter dem Titel Subjektives Wohlbefinden – ein Graben zwischen Stadt und Land? Die Stadt-Land-Differenz im subjektiven Wohlbefinden der Schweizer Bevölkerung und der Einfluss des individuellen Sozialkapitals, habe ich meine Bachelorarbeit im Herbstsemester 2021 verfasst. Darin habe ich mich mit dem Sozialkapital in der Schweiz zwischen Stadt und Land beschäftigt.\nDirekt zum Poster."
  },
  {
    "objectID": "projects/twitter_project/files_twitter_project/tweet_scraping_demo_code.html",
    "href": "projects/twitter_project/files_twitter_project/tweet_scraping_demo_code.html",
    "title": "Lisa Götschi",
    "section": "",
    "text": "import tweepy\nimport time\nimport pandas as pd\n\n\nconsumer_key = \"add_key\"\nconsumer_secret = \"add_key\"\naccess_key = \"add_key\"\naccess_secret = \"add_key\"\nbearer_token = \"add_key\"\n\n\nclient = tweepy.Client(bearer_token, wait_on_rate_limit=True)\n\n\n\nraw_tweets = []\nfor response in tweepy.Paginator(client.search_all_tweets, \n                                 query = '\"ukraine krieg\" -is:retweet lang:de',\n                                 tweet_fields = ['created_at', 'geo', 'text'],\n                                 start_time = '2022-02-28T00:00:00Z',\n                                 end_time = '2022-11-30T00:00:00Z'\n                                  ):\n    time.sleep(1)\n    raw_tweets.append(response)\n\n\nfor response in tweepy.Paginator(client.search_all_tweets, \n                                 query = '\"ukraine krieg\" -is:retweet lang:de',\n                                 tweet_fields = ['created_at', 'geo', 'text'],\n                                 start_time = '2022-08-24T08:23:52Z',\n                                 end_time = '2022-11-30T00:00:00Z'\n                                  ):\n    time.sleep(1)\n    raw_tweets.append(response)\n\n\nfor response in tweepy.Paginator(client.search_all_tweets, \n                                 query = '\"ukraine krieg\" -is:retweet lang:de',\n                                 tweet_fields = ['created_at', 'geo', 'text'],\n                                 start_time = '2022-09-22T06:14:34Z',\n                                 end_time = '2022-11-30T00:00:00Z'\n                                  ):\n    time.sleep(1)\n    raw_tweets.append(response)\n\n\nlen(raw_tweets)  \n\n\nraw_tweets2 = []\nfor response in tweepy.Paginator(client.search_all_tweets, \n                                 query = '\"ukraine krieg\" -is:retweet lang:de',\n                                 tweet_fields = ['created_at', 'geo', 'text'],\n                                 start_time = '2022-02-28T00:00:00Z',\n                                 end_time = '2022-08-24T08:23:52Z'\n                                  ):\n    time.sleep(1)\n    raw_tweets2.append(response)\n\n\nraw_tweets3 = []\nfor response in tweepy.Paginator(client.search_all_tweets, \n                                 query = '\"ukraine krieg\" -is:retweet lang:de',\n                                 tweet_fields = ['created_at', 'geo', 'text'],\n                                 start_time = '2022-02-28T00:00:00Z',\n                                 end_time = '2022-06-15T20:53:28Z'\n                                  ):\n    time.sleep(1)\n    raw_tweets3.append(response)\n\n\nraw_tweets4 = []\nfor response in tweepy.Paginator(client.search_all_tweets, \n                                 query = '\"ukraine krieg\" -is:retweet lang:de',\n                                 tweet_fields = ['created_at', 'geo', 'text'],\n                                 start_time = '2022-02-28T00:00:00Z',\n                                 end_time = '2022-03-04T05:46:41Z'\n                                  ):\n    time.sleep(1)\n    raw_tweets4.append(response)\n\n\nraw_tweets5 = []\nfor response in tweepy.Paginator(client.search_all_tweets, \n                                 query = '\"ukraine krieg\" -is:retweet lang:de',\n                                 tweet_fields = ['created_at', 'geo', 'text'],\n                                 start_time = '2022-02-24T23:59:59Z',\n                                 end_time = '2022-03-01T08:36:47Z'\n                                  ):\n    time.sleep(1)\n    raw_tweets5.append(response)\n\n\nresult = []\nuser_dict = {}\n# Loop through each response object\nfor response in raw_tweets:\n    for tweet in response.data:\n        # Put all of the information we want to keep in a single dictionary for each tweet\n        result.append({\n                       'text': tweet.text,\n                       'created_at': tweet.created_at,\n                       'geo': tweet.geo\n                      })\n\n# Change this list of dictionaries into a dataframe\ndf = pd.DataFrame(result)\n\n\nresult2 = []\nuser_dict2 = {}\n# Loop through each response object\nfor response in raw_tweets2:\n    for tweet in response.data:\n        # Put all of the information we want to keep in a single dictionary for each tweet\n        result2.append({\n                       'text': tweet.text,\n                       'created_at': tweet.created_at,\n                       'geo': tweet.geo\n                      })\n\n# Change this list of dictionaries into a dataframe\ndf2 = pd.DataFrame(result2)\n\n\ndf2.to_csv('uk_tag4.csv', encoding='utf-8')\n\n\nlen(raw_tweets)\n\n\nresult3 = []\nuser_dict3 = {}\n# Loop through each response object\nfor response in raw_tweets3:\n    for tweet in response.data:\n        # Put all of the information we want to keep in a single dictionary for each tweet\n        result3.append({\n                       'text': tweet.text,\n                       'created_at': tweet.created_at,\n                       'geo': tweet.geo\n                      })\n\n# Change this list of dictionaries into a dataframe\ndf3 = pd.DataFrame(result3)\n\n\ndf3.to_csv('uk_tag5.csv', encoding='utf-8')\n\n\nresult4 = []\nuser_dict4 = {}\n# Loop through each response object\nfor response in raw_tweets4:\n    for tweet in response.data:\n        # Put all of the information we want to keep in a single dictionary for each tweet\n        result4.append({\n                       'text': tweet.text,\n                       'created_at': tweet.created_at,\n                       'geo': tweet.geo\n                      })\n\n# Change this list of dictionaries into a dataframe\ndf4 = pd.DataFrame(result4)\n\n\ndf4.to_csv('uk_tag6.csv', encoding='utf-8')\n\n\nresult5 = []\nuser_dict5 = {}\n# Loop through each response object\nfor response in raw_tweets5:\n    for tweet in response.data:\n        # Put all of the information we want to keep in a single dictionary for each tweet\n        result5.append({\n                       'text': tweet.text,\n                       'created_at': tweet.created_at,\n                       'geo': tweet.geo\n                      })\n\n# Change this list of dictionaries into a dataframe\ndf5 = pd.DataFrame(result5)\n\n\ndf5.to_csv('uk_tag7.csv', encoding='utf-8')\n\n\nraw_tweets6 = []\nfor response in tweepy.Paginator(client.search_all_tweets, \n                                 query = '\"ukraine krieg\" -is:retweet lang:de',\n                                 tweet_fields = ['created_at', 'geo', 'text'],\n                                 start_time = '2022-02-24T00:00:00Z',\n                                 end_time = '2022-02-25T00:00:58Z'\n                                  ):\n    time.sleep(1)\n    raw_tweets6.append(response)\n\n\nresult6 = []\nuser_dict6 = {}\n# Loop through each response object\nfor response in raw_tweets6:\n    for tweet in response.data:\n        # Put all of the information we want to keep in a single dictionary for each tweet\n        result6.append({\n                       'text': tweet.text,\n                       'created_at': tweet.created_at,\n                       'geo': tweet.geo\n                      })\n\n# Change this list of dictionaries into a dataframe\ndf6 = pd.DataFrame(result6)\n\n\ndf6.to_csv('uk_tag8.csv', encoding='utf-8')\n\n\nraw_tweets7 = []\nfor response in tweepy.Paginator(client.search_all_tweets, \n                                 query = '\"ukraine krieg\" -is:retweet lang:de',\n                                 tweet_fields = ['created_at', 'geo', 'text'],\n                                 start_time = '2022-08-24T08:23:16Z',\n                                 end_time = '2022-09-22T06:14:34Z'\n                                  ):\n    time.sleep(1)\n    raw_tweets7.append(response)\n\n\nresult7 = []\nuser_dict7 = {}\n# Loop through each response object\nfor response in raw_tweets7:\n    for tweet in response.data:\n        # Put all of the information we want to keep in a single dictionary for each tweet\n        result7.append({\n                       'text': tweet.text,\n                       'created_at': tweet.created_at,\n                       'geo': tweet.geo\n                      })\n\n# Change this list of dictionaries into a dataframe\ndf7 = pd.DataFrame(result7)\n\n\ndf7.to_csv('uk_tag2.csv', encoding='utf-8')\n\n\nraw_tweets8 = []\nfor response in tweepy.Paginator(client.search_all_tweets, \n                                 query = '\"ukraine krieg\" -is:retweet lang:de',\n                                 tweet_fields = ['created_at', 'geo', 'text'],\n                                 start_time = '2022-11-29T23:57:32Z',\n                                 end_time = '2022-11-30T23:59:59Z'\n                                  ):\n    time.sleep(1)\n    raw_tweets8.append(response)\n\n\nresult8 = []\nuser_dict8 = {}\n# Loop through each response object\nfor response in raw_tweets8:\n    for tweet in response.data:\n        # Put all of the information we want to keep in a single dictionary for each tweet\n        result8.append({\n                       'text': tweet.text,\n                       'created_at': tweet.created_at,\n                       'geo': tweet.geo\n                      })\n\n# Change this list of dictionaries into a dataframe\ndf8 = pd.DataFrame(result8)\n\n\ndf8.to_csv('uk_tag1.csv', encoding='utf-8')\n\n\n# Added from differerent notebook the example preparation and analysis of the first week \n\n\nimport nltk\nimport germansentiment \nfrom nltk.probability import FreqDist\nfrom textblob_de import TextBlobDE\nimport csv\nimport string \nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom nltk.tokenize import sent_tokenize\nnltk.download('punkt')\nfrom textblob_de import TextBlobDE as TextBlob\n\n\nwith open('/Users/lisa/Final_Project_geopolitcs/first_week.csv', 'r') as csv_datei:\n    reader = csv.reader(csv_datei, delimiter=',')\n    text = csv_datei.read()\n    token_text = sent_tokenize(text)\n    words = text.split()\n\n\nlowercase_words = []\nfor w in words: lowercase_words.append(w.lower()) \nprint(len(lowercase_words)) \n\n\ndef remove_punc(string):\n    punc = '''!()-[]{};:'\"\\, &lt;&gt;./?@#$%^&*_~'''\n    for ele in string:  \n        if ele in punc:  \n            string = string.replace(ele, \"\") \n    return string\n \nlowercase_words_clean = [remove_punc(i) for i in lowercase_words]\n\n\ntweet_words = []\n\nfor word in lowercase_words_clean:\n    if word.startswith('http'):\n        word = \"http\"\n    elif word.startswith('@'):\n        word = '@user'\n    elif word: \n        word = word\n        tweet_words.append(word)\n\n\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstopwords = stopwords.words('german')\n\n\nother_stopwords = [\"ukraine\", \"ukraine-krieg\", \"mehr\", \"-\", \"via\", \"+++\", \"ukrainekrieg\", \"krieg\", \"tonline\", \"@user\", \"http\"]\nprint(other_stopwords)\n\n\nwords_withoutstop = []\nfor word in tweet_words :\n    if word not in stopwords:\n        words_withoutstop.append(word)\n\nfdist = FreqDist(words_withoutstop)\nfdist.plot(10)\n\nprint(len(words), len(words_withoutstop))\n\n\nwords_withoutstop2 = []\nfor word in words_withoutstop:\n    if word not in other_stopwords:\n        words_withoutstop2.append(word)\nfdist2 = FreqDist(words_withoutstop2)\nfdist2.plot(10)\n\n\nprint(len(words_withoutstop), len(words_withoutstop2))\n\n\nstring_firstweek = ''\nfor x in words_withoutstop2:\n    string_firstweek += ' ' + x\n\n\nwc = WordCloud(scale=3,\n                colormap='Paired',\n                background_color='white')\nwc.generate(string_firstweek)\n\nplt.imshow(wc)\nwc.to_file('wordcloud_firstweek.png')\nplt.axis(\"off\")\n\n\nblob = TextBlob(string_firstweek)\n\nprint(blob.sentiment)\n\nsentiment_mw = blob.sentiment"
  },
  {
    "objectID": "projects/twitter_project/index.html",
    "href": "projects/twitter_project/index.html",
    "title": "Sentiment Analysis Twitter Daten zum Ukrainekrieg 2022",
    "section": "",
    "text": "Im Herbstsemester 2022 habe ich im Rahmen eines Seminars Political Geographies eine Sentimentanalysis deutschsprachiger Tweets zum Ukrainekrieg durchgeführt. Die grundlegende Idee war es, Yashins Konzept des “public cynism” (Navaro-Yashin 2002) ausfindig zu machen. Grob dabei geht es um eine Art Ressentiment, welches sich nach einer Zeit der Aufruhr einstellt. Für den vorliegenden Fall wäre zu erwarten, dass eine grosse Anzahl Tweets abgesetzt wird nach wichtigen Ereignissen, diese aber nach und nach wieder abnehmen.\nDafür habe ich in einem ersten Schritt Tweets über den Untersuchungszeitraum hinweg über die Twitter Research API gescraped. Anschliessend in kleineren Teilen die Daten für die Sentimentanalysis und weiteren Untersuchungen vorbereitet.\n\n\nDie aus der Analyse hervorgehenden Schlussfolgerungen der Ergebnisse.\nDie Ergebnisse sind eher explorativ und ein erster Einblick in das, was möglich ist, es gibt wahrscheinlich noch viel mehr zu finden. Ich wüsste nicht, wie ich das tun sollte, da ich nur über begrenzte Kenntnisse verfüge.\nDie Analyse gibt jedoch gute Anhaltspunkte für weitere Untersuchungen und Optimierungen, wie z. B. die Durchführung einer Konkordanz Analyse, bei der untersucht wird, welche Wörter zusammen vorkommen, aber auch das Entfernen von Nachrichtenagenturen aus der Quelle könnte ein klareres Bild des öffentlichen Zynismus ergeben.\n\n\n\nSource: tweet_scraping_demo_code.ipynb\n\n\n\nInnerhalb von Monaten ändert sich die Häufigkeit, aber es ist ein Trend erkennbar. Der auf ein abnehmendes Interesse hin deutet. Die Gegenoffensive der Ukraine im August → machte das Thema wieder sichtbar.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nNavaro-Yashin, Yael. 2002. Faces of the State: Secularism and Public Life in Turkey. Princeton, N.J: Princeton University Press."
  },
  {
    "objectID": "projects/cia/index.html",
    "href": "projects/cia/index.html",
    "title": "fragnach - Quantitative Unterstützung Studie ‘Cannabis im Alltag’",
    "section": "",
    "text": "Innerhalb des zusammen mit Swassthi Sarma gegründeten Unternehmen fragnach habe ich das Forschungsteam Unexplored Realities der Universität St.Gallen in ihrer Studie zum Cannabiskonsum im Alltag unterstützen dürfen.\nNeben der Anpassung der Umfrage und deskriptiven statistischen Analysen habe ich in einer explorativen Clusteranalyse vorläufige Ergebnisse zu möglichen Konsumgruppen innerhalb der Konsument:innen herausarbeiten können. Diese durfte ich im Mai 2023 an einem Forschungsgespräch zur Zukunft der Cannabisforschung an der Universität St.Gallen vorstellen."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projekte",
    "section": "",
    "text": "fragnach - Quantitative Unterstützung Studie ‘Cannabis im Alltag’\n\n\n\nacademic\n\n\nresearch\n\n\nfragnach\n\n\n\n\n\n\n\nLisa Götschi\n\n\nMay 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSentiment Analysis Twitter Daten zum Ukrainekrieg 2022\n\n\n\nacademic\n\n\nproject\n\n\ncode\n\n\n\n\n\n\n\nLisa Götschi\n\n\nDec 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBachelorarbeit und Poster\n\n\n\nacademic\n\n\nwriting\n\n\n\n\n\n\n\nLisa Götschi\n\n\nSep 1, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Über mich",
    "section": "",
    "text": "Nach meinem Bachelor in Soziologie und Geografie in Zürich habe ich meinen Master in Soziologie an der Universität Bern begonnen. Sowie ein Gaststudium an der Universität Basel absolviert. Durch meine frühere Tätigkeit als Tutorin und die Arbeit als wissenschaftliche Hilfsassistentin hat sich mein Interesse an quantitativer Datenanalyse und der Wissenschaft deutlich herauskristallisiert.\nNeben quantitativen Methoden interessieren mich innerhalb der Soziologie besonders Sozialkapitaltheorien, soziale Ungleichheit und Fragestellungen, die sich (on- & offline) intersektional mit räumlichen und sozioökonomischen Aspekten befassen.\n\n\n\n   CV\n  Projekte"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "Download"
  },
  {
    "objectID": "cv.html#ausbildung",
    "href": "cv.html#ausbildung",
    "title": "CV",
    "section": "Ausbildung",
    "text": "Ausbildung\n\nMaster of Arts in Soziologie Universität Bern Sep. 2022 - heute\nMono: Soziologie\nmit Gaststudium an der Universität Basel von Sep. 2023 bis Jul. 2024\n\nBachelor of Arts in Sozialwissenschaften Universität Zürich Sep. 2018 - Jul. 2022\nMajor: Soziologie und Minor: Geographie\nAbschlussnote: 5.1\nBachelorarbeit: Subjektives Wohlbefinden – ein Graben zwischen Stadt und Land?\n\nGymnasiale Matur Gymnasium am Münsterplatz Basel Aug. 2014 - Jun. 2017\nBilinguale Matur (D/E) & International Baccalaureate (IB)\nSchwerpunkt: Englisch\nMaturarbeit: Sustainable Fashion - Comparison of two brands H&M conscious and Coop Naturaline."
  },
  {
    "objectID": "cv.html#arbeitserfahrung",
    "href": "cv.html#arbeitserfahrung",
    "title": "CV",
    "section": "Arbeitserfahrung",
    "text": "Arbeitserfahrung\n\nCo-founder fragnach Dez 2020 bis heute\nfragnach.ch Zürich\nQuantitative Methodenberatung und Projektbetreuung\nErstellen von Umfragen\nRecherche von Theorien und Studien\n\nHilfsassistentin im Projekt: “The neighborhood in the cloud” Mai 2022 bis Jul. 2024\nUniversität Bern - Institut für Soziologie\nUnterstützung in der Umsetzung des Projektes z.B. Daten- und Literaturrecherche, Aufbereiten von Daten, eigenständige quantitative Analysen.\n\nTutorin für Techniken wissenschaftlichen Arbeitens Sep. 2021 bis Feb. 2022\nUniversität Zürich\nLeiten und organisieren von Tutoraten für Studierende & Leistungsnachweiskorrekturen\n\nTutorin für empirische Sozialforschung und Umfragemethoden Sep. 2020 bis Jun. 2022\nUniversität Zürich\nLeiten und organisieren von Tutoraten für Studierende & Leistungsnachweiskorrekturen, Gruppenbetreuung"
  },
  {
    "objectID": "cv.html#weitere-erfahrung",
    "href": "cv.html#weitere-erfahrung",
    "title": "CV",
    "section": "Weitere Erfahrung",
    "text": "Weitere Erfahrung\n\n\nHilfsassistentin bei Dr. Robert Schäfer Dez. 2023\nUniversität Basel - Departement Gesellschaftswissenschaften\nKodieren von Interviews mit ATLAS.ti\n\nTeilnehmerin Fundraising Academy Jan. 2021 - Apr. 2021\nDigital Innovation Lab AG, Bern\nLernen von Strategien zum Start-up Aufbau, Investorenfindung\nErstellen von Businessplan und Marketingstrategien\n\nSoziologie-Fachverein Vorstandsmitglied Sep. 2019 - Sep. 2021\nUniversität Zürich\nVerantwortlich für Design und Druck von Postern, Flyern etc. & Eventorganisation"
  },
  {
    "objectID": "cv.html#it-kompetenzen",
    "href": "cv.html#it-kompetenzen",
    "title": "CV",
    "section": "IT Kompetenzen",
    "text": "IT Kompetenzen\n\n\n\nStatistiksoftware R & STATA\nsehr gute Kenntnisse\n\n\nMicrosoft Office (Word, Powerpoint und Excel)\nsehr gute Kenntnisse\n\n\nAdobe InDesign, Illustrator\ngute Kenntnisse\n\n\nSNAP & QGIS\nGrundkentnisse\n\n\nWordPress, CSM Magnolia\nGrundkentnisse"
  },
  {
    "objectID": "cv.html#sprachen",
    "href": "cv.html#sprachen",
    "title": "CV",
    "section": "Sprachen",
    "text": "Sprachen\n\n\n\nDeutsch\nMuttersprache\n\n\nEnglisch\nZweisprachige Matur (C1)\n\n\nFranzösisch\nGrundlagenkenntisse (A1)\n\n\nItalienisch\nGrundlagenkenntisse (A1)"
  },
  {
    "objectID": "cv.html#hobbys-und-interessen",
    "href": "cv.html#hobbys-und-interessen",
    "title": "CV",
    "section": "Hobbys und Interessen",
    "text": "Hobbys und Interessen\n\nMit Yoga und ausgiebigen Spaziergängen mit meinem Hund halte ich mich fit. Meditation und Häkeln geben mir einen Ausgleich zum Arbeitsalltag und durch das Lesen von Sachliteratur wie auch Romanen erweitere ich meinen Wissenshorizont."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "No matching items"
  }
]