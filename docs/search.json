[
  {
    "objectID": "projects/bachelor_arbeit/index.html",
    "href": "projects/bachelor_arbeit/index.html",
    "title": "Bachelorarbeit und Poster",
    "section": "",
    "text": "Unter dem Titel Subjektives Wohlbefinden – ein Graben zwischen Stadt und Land? Die Stadt-Land-Differenz im subjektiven Wohlbefinden der Schweizer Bevölkerung und der Einfluss des individuellen Sozialkapitals habe ich meine Bachelorarbeit im Herbstsemester 2021 verfasst.\nDarin habe ich mich mit dem subjektiven Wohlbefinden der Stadt- und Landbevölkerung der Schweiz beschäftigt, um mögliche Unterschiede anhand der Art des Sozialkapitals zu erklären.\nIn «entwickelten» Ländern wurde beobachtet, dass «Personen aus ländlichen Gebieten tendenziell ein höheres subjektives Wohlbefinden als Stadtbewohner:innen haben» (Sørensen 2021), bezeichnet wird dies als «Rural Happiness Paradox».\nFür die Schweiz konnte ich es in diesem Fall nicht bestätigen. Eine mögliche Erklärung könnte sein, dass die Schweiz im Vergleich zu anderen Ländern kleinere «Grossstädte» hat, was auch den Unterschied zwischen Stadt und Land geringer erscheinen lässt.\nDirekt zum Poster.\n\n\n\n\n\nLiteratur\n\nSørensen, Jens F. L. 2021. „The Rural Happiness Paradox in Developed Countries“. Social Science Research 98 (August): 102581. https://doi.org/10.1016/j.ssresearch.2021.102581."
  },
  {
    "objectID": "projects/besserwisserin/index.html",
    "href": "projects/besserwisserin/index.html",
    "title": "Podcast - Besserwisserin (pausiert)",
    "section": "",
    "text": "Ein kleines Herzensprojekt, für welches ich derzeit leider nicht die Ressourcen habe. Ende 2023 habe ich zusammen mit meiner Co-Host den Podcast Besserwisserin gestartet. In diesem sprechen wir aus soziologischer Perspektive und von unseren persönlichen Erfahrungen ausgehend über gesellschaftliche Themen, welche uns beschäftigen und wir uns genauer anschauen möchten.\nNach unseren drei Testfolgen haben wir gemerkt, dass dieses Projekt, um es für unsere Ansprüche gerecht aufzubauen, mehr Zeit in Anspruch nimmt als erwartet. Das Projekt wird so bald als möglich wieder aufgenommen werden.\n\n\nDie ersten drei Folgen:"
  },
  {
    "objectID": "projects/twitter_project/files_twitter_project/tweet_scraping_demo_code.html",
    "href": "projects/twitter_project/files_twitter_project/tweet_scraping_demo_code.html",
    "title": "Lisa Götschi",
    "section": "",
    "text": "import tweepy\nimport time\nimport pandas as pd\n\n\nconsumer_key = \"add_key\"\nconsumer_secret = \"add_key\"\naccess_key = \"add_key\"\naccess_secret = \"add_key\"\nbearer_token = \"add_key\"\n\n\nclient = tweepy.Client(bearer_token, wait_on_rate_limit=True)\n\n\n\nraw_tweets = []\nfor response in tweepy.Paginator(client.search_all_tweets, \n                                 query = '\"ukraine krieg\" -is:retweet lang:de',\n                                 tweet_fields = ['created_at', 'geo', 'text'],\n                                 start_time = '2022-02-28T00:00:00Z',\n                                 end_time = '2022-11-30T00:00:00Z'\n                                  ):\n    time.sleep(1)\n    raw_tweets.append(response)\n\n\nfor response in tweepy.Paginator(client.search_all_tweets, \n                                 query = '\"ukraine krieg\" -is:retweet lang:de',\n                                 tweet_fields = ['created_at', 'geo', 'text'],\n                                 start_time = '2022-08-24T08:23:52Z',\n                                 end_time = '2022-11-30T00:00:00Z'\n                                  ):\n    time.sleep(1)\n    raw_tweets.append(response)\n\n\nfor response in tweepy.Paginator(client.search_all_tweets, \n                                 query = '\"ukraine krieg\" -is:retweet lang:de',\n                                 tweet_fields = ['created_at', 'geo', 'text'],\n                                 start_time = '2022-09-22T06:14:34Z',\n                                 end_time = '2022-11-30T00:00:00Z'\n                                  ):\n    time.sleep(1)\n    raw_tweets.append(response)\n\n\nlen(raw_tweets)  \n\n\nraw_tweets2 = []\nfor response in tweepy.Paginator(client.search_all_tweets, \n                                 query = '\"ukraine krieg\" -is:retweet lang:de',\n                                 tweet_fields = ['created_at', 'geo', 'text'],\n                                 start_time = '2022-02-28T00:00:00Z',\n                                 end_time = '2022-08-24T08:23:52Z'\n                                  ):\n    time.sleep(1)\n    raw_tweets2.append(response)\n\n\nraw_tweets3 = []\nfor response in tweepy.Paginator(client.search_all_tweets, \n                                 query = '\"ukraine krieg\" -is:retweet lang:de',\n                                 tweet_fields = ['created_at', 'geo', 'text'],\n                                 start_time = '2022-02-28T00:00:00Z',\n                                 end_time = '2022-06-15T20:53:28Z'\n                                  ):\n    time.sleep(1)\n    raw_tweets3.append(response)\n\n\nraw_tweets4 = []\nfor response in tweepy.Paginator(client.search_all_tweets, \n                                 query = '\"ukraine krieg\" -is:retweet lang:de',\n                                 tweet_fields = ['created_at', 'geo', 'text'],\n                                 start_time = '2022-02-28T00:00:00Z',\n                                 end_time = '2022-03-04T05:46:41Z'\n                                  ):\n    time.sleep(1)\n    raw_tweets4.append(response)\n\n\nraw_tweets5 = []\nfor response in tweepy.Paginator(client.search_all_tweets, \n                                 query = '\"ukraine krieg\" -is:retweet lang:de',\n                                 tweet_fields = ['created_at', 'geo', 'text'],\n                                 start_time = '2022-02-24T23:59:59Z',\n                                 end_time = '2022-03-01T08:36:47Z'\n                                  ):\n    time.sleep(1)\n    raw_tweets5.append(response)\n\n\nresult = []\nuser_dict = {}\n# Loop through each response object\nfor response in raw_tweets:\n    for tweet in response.data:\n        # Put all of the information we want to keep in a single dictionary for each tweet\n        result.append({\n                       'text': tweet.text,\n                       'created_at': tweet.created_at,\n                       'geo': tweet.geo\n                      })\n\n# Change this list of dictionaries into a dataframe\ndf = pd.DataFrame(result)\n\n\nresult2 = []\nuser_dict2 = {}\n# Loop through each response object\nfor response in raw_tweets2:\n    for tweet in response.data:\n        # Put all of the information we want to keep in a single dictionary for each tweet\n        result2.append({\n                       'text': tweet.text,\n                       'created_at': tweet.created_at,\n                       'geo': tweet.geo\n                      })\n\n# Change this list of dictionaries into a dataframe\ndf2 = pd.DataFrame(result2)\n\n\ndf2.to_csv('uk_tag4.csv', encoding='utf-8')\n\n\nlen(raw_tweets)\n\n\nresult3 = []\nuser_dict3 = {}\n# Loop through each response object\nfor response in raw_tweets3:\n    for tweet in response.data:\n        # Put all of the information we want to keep in a single dictionary for each tweet\n        result3.append({\n                       'text': tweet.text,\n                       'created_at': tweet.created_at,\n                       'geo': tweet.geo\n                      })\n\n# Change this list of dictionaries into a dataframe\ndf3 = pd.DataFrame(result3)\n\n\ndf3.to_csv('uk_tag5.csv', encoding='utf-8')\n\n\nresult4 = []\nuser_dict4 = {}\n# Loop through each response object\nfor response in raw_tweets4:\n    for tweet in response.data:\n        # Put all of the information we want to keep in a single dictionary for each tweet\n        result4.append({\n                       'text': tweet.text,\n                       'created_at': tweet.created_at,\n                       'geo': tweet.geo\n                      })\n\n# Change this list of dictionaries into a dataframe\ndf4 = pd.DataFrame(result4)\n\n\ndf4.to_csv('uk_tag6.csv', encoding='utf-8')\n\n\nresult5 = []\nuser_dict5 = {}\n# Loop through each response object\nfor response in raw_tweets5:\n    for tweet in response.data:\n        # Put all of the information we want to keep in a single dictionary for each tweet\n        result5.append({\n                       'text': tweet.text,\n                       'created_at': tweet.created_at,\n                       'geo': tweet.geo\n                      })\n\n# Change this list of dictionaries into a dataframe\ndf5 = pd.DataFrame(result5)\n\n\ndf5.to_csv('uk_tag7.csv', encoding='utf-8')\n\n\nraw_tweets6 = []\nfor response in tweepy.Paginator(client.search_all_tweets, \n                                 query = '\"ukraine krieg\" -is:retweet lang:de',\n                                 tweet_fields = ['created_at', 'geo', 'text'],\n                                 start_time = '2022-02-24T00:00:00Z',\n                                 end_time = '2022-02-25T00:00:58Z'\n                                  ):\n    time.sleep(1)\n    raw_tweets6.append(response)\n\n\nresult6 = []\nuser_dict6 = {}\n# Loop through each response object\nfor response in raw_tweets6:\n    for tweet in response.data:\n        # Put all of the information we want to keep in a single dictionary for each tweet\n        result6.append({\n                       'text': tweet.text,\n                       'created_at': tweet.created_at,\n                       'geo': tweet.geo\n                      })\n\n# Change this list of dictionaries into a dataframe\ndf6 = pd.DataFrame(result6)\n\n\ndf6.to_csv('uk_tag8.csv', encoding='utf-8')\n\n\nraw_tweets7 = []\nfor response in tweepy.Paginator(client.search_all_tweets, \n                                 query = '\"ukraine krieg\" -is:retweet lang:de',\n                                 tweet_fields = ['created_at', 'geo', 'text'],\n                                 start_time = '2022-08-24T08:23:16Z',\n                                 end_time = '2022-09-22T06:14:34Z'\n                                  ):\n    time.sleep(1)\n    raw_tweets7.append(response)\n\n\nresult7 = []\nuser_dict7 = {}\n# Loop through each response object\nfor response in raw_tweets7:\n    for tweet in response.data:\n        # Put all of the information we want to keep in a single dictionary for each tweet\n        result7.append({\n                       'text': tweet.text,\n                       'created_at': tweet.created_at,\n                       'geo': tweet.geo\n                      })\n\n# Change this list of dictionaries into a dataframe\ndf7 = pd.DataFrame(result7)\n\n\ndf7.to_csv('uk_tag2.csv', encoding='utf-8')\n\n\nraw_tweets8 = []\nfor response in tweepy.Paginator(client.search_all_tweets, \n                                 query = '\"ukraine krieg\" -is:retweet lang:de',\n                                 tweet_fields = ['created_at', 'geo', 'text'],\n                                 start_time = '2022-11-29T23:57:32Z',\n                                 end_time = '2022-11-30T23:59:59Z'\n                                  ):\n    time.sleep(1)\n    raw_tweets8.append(response)\n\n\nresult8 = []\nuser_dict8 = {}\n# Loop through each response object\nfor response in raw_tweets8:\n    for tweet in response.data:\n        # Put all of the information we want to keep in a single dictionary for each tweet\n        result8.append({\n                       'text': tweet.text,\n                       'created_at': tweet.created_at,\n                       'geo': tweet.geo\n                      })\n\n# Change this list of dictionaries into a dataframe\ndf8 = pd.DataFrame(result8)\n\n\ndf8.to_csv('uk_tag1.csv', encoding='utf-8')\n\n\n# Added from differerent notebook the example preparation and analysis of the first week \n\n\nimport nltk\nimport germansentiment \nfrom nltk.probability import FreqDist\nfrom textblob_de import TextBlobDE\nimport csv\nimport string \nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom nltk.tokenize import sent_tokenize\nnltk.download('punkt')\nfrom textblob_de import TextBlobDE as TextBlob\n\n\nwith open('/Users/lisa/Final_Project_geopolitcs/first_week.csv', 'r') as csv_datei:\n    reader = csv.reader(csv_datei, delimiter=',')\n    text = csv_datei.read()\n    token_text = sent_tokenize(text)\n    words = text.split()\n\n\nlowercase_words = []\nfor w in words: lowercase_words.append(w.lower()) \nprint(len(lowercase_words)) \n\n\ndef remove_punc(string):\n    punc = '''!()-[]{};:'\"\\, &lt;&gt;./?@#$%^&*_~'''\n    for ele in string:  \n        if ele in punc:  \n            string = string.replace(ele, \"\") \n    return string\n \nlowercase_words_clean = [remove_punc(i) for i in lowercase_words]\n\n\ntweet_words = []\n\nfor word in lowercase_words_clean:\n    if word.startswith('http'):\n        word = \"http\"\n    elif word.startswith('@'):\n        word = '@user'\n    elif word: \n        word = word\n        tweet_words.append(word)\n\n\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstopwords = stopwords.words('german')\n\n\nother_stopwords = [\"ukraine\", \"ukraine-krieg\", \"mehr\", \"-\", \"via\", \"+++\", \"ukrainekrieg\", \"krieg\", \"tonline\", \"@user\", \"http\"]\nprint(other_stopwords)\n\n\nwords_withoutstop = []\nfor word in tweet_words :\n    if word not in stopwords:\n        words_withoutstop.append(word)\n\nfdist = FreqDist(words_withoutstop)\nfdist.plot(10)\n\nprint(len(words), len(words_withoutstop))\n\n\nwords_withoutstop2 = []\nfor word in words_withoutstop:\n    if word not in other_stopwords:\n        words_withoutstop2.append(word)\nfdist2 = FreqDist(words_withoutstop2)\nfdist2.plot(10)\n\n\nprint(len(words_withoutstop), len(words_withoutstop2))\n\n\nstring_firstweek = ''\nfor x in words_withoutstop2:\n    string_firstweek += ' ' + x\n\n\nwc = WordCloud(scale=3,\n                colormap='Paired',\n                background_color='white')\nwc.generate(string_firstweek)\n\nplt.imshow(wc)\nwc.to_file('wordcloud_firstweek.png')\nplt.axis(\"off\")\n\n\nblob = TextBlob(string_firstweek)\n\nprint(blob.sentiment)\n\nsentiment_mw = blob.sentiment"
  },
  {
    "objectID": "projects/twitter_project/index.html",
    "href": "projects/twitter_project/index.html",
    "title": "Sentiment Analysis Twitter Daten zum Ukrainekrieg 2022",
    "section": "",
    "text": "Im Herbstsemester 2022 habe ich im Rahmen eines Seminars Political Geographies eine Sentiment Analysis deutschsprachiger Tweets zum Ukrainekrieg durchgeführt. Die grundlegende Idee war es, Yashins Konzept des “public cynism” (Navaro-Yashin 2002) ausfindig zu machen. ausfindig zu machen. Kurzum bezeichnet es eine Art Ressentiment, welches sich nach einer Zeit der Aufruhr einstellt. Für den vorliegenden Fall wäre zu erwarten, dass eine grosse Anzahl Tweets abgesetzt wird nach wichtigen Ereignissen, diese aber nach und nach abnehmen und das Thema an Bedeutung verliert.\nDafür habe ich in einem ersten Schritt Tweets über den Untersuchungszeitraum hinweg mit der Twitter Research API gescraped um anschliessend die Tweets in kleinere Datensets aufgeteilt für die Sentiment Analysis und weiteren Untersuchungen vorzubereiten.\n\n\nDie aus der Analyse hervorgehenden Schlussfolgerungen der Ergebnisse.\nDie Ergebnisse sind explorativ und geben einen ersten Einblick in das, was möglich ist. Wahrscheinlich liesse sich noch weitaus mehr Information aus den Daten gewinnen. Da ich nur über begrenzte Kenntnisse innerhalb der statistischen Textanalyse verfüge, beschränken sich die Berechnungen auf die Tweethäufigkeit, die Polarität sowie Sentiment Analyse.\nDie Analyse gibt jedoch gute Anhaltspunkte für weitere Untersuchungen und Optimierungen, wie z. B. die Durchführung einer Konkordanz Analyse, bei der untersucht wird, welche Wörter zusammen vorkommen, aber auch das Entfernen von Nachrichtenagenturen aus der Quelle könnte ein klareres Bild des “public cynism” ergeben.\n\n\n\nQuelle: tweet_scraping_demo_code.ipynb\n\n\n\nInnerhalb von Monaten ändert sich die Tweethäufigkeit. Allerdings lässt sich über die Zeit eine Abnahme der Häufigkeit erkennen. Dieser Trend deutet auf ein abnehmendes Interesse hin, welcher mit der erwarteten Auswirkung von “public cynism” einhergeht. Ersichtlich ist auch die erneut zunehmende Häufigkeit im August, womöglich ausgelöst durch die Gegenoffensive der Ukraine in diesem Monat. Das Thema des Ukrainekriegs gewinnt wieder an Aktualität.\n\n\n\n\n\nDas Zählen der negativen und positiven Wörter aller Tweets eines Monats zeigt, dass auf beide Seiten eine ausgeglichene Verteilung besteht.\n\n\n\n\n\nDiese Neutralität, die in den Tweets besteht, lässt sich ebenfalls auch im Vergleich von drei gewählten Wochen innerhalb des Untersuchungszeitraumes finden. Die Werte liegen alle nahe bei null, wobei das Sentiment in der letzten Woche verglichen zu den anderen beiden Zeitpunkten am negativsten ausfällt.\n\n\n\n\n\n\nLiteratur\n\nNavaro-Yashin, Yael. 2002. Faces of the state: secularism and public life in Turkey. Princeton, N.J: Princeton University Press."
  },
  {
    "objectID": "projects/cia/index.html",
    "href": "projects/cia/index.html",
    "title": "fragnach - Quantitative Unterstützung Studie ‘Cannabis im Alltag’",
    "section": "",
    "text": "Innerhalb des zusammen mit Swassthi Sarma gegründeten Unternehmen fragnach habe ich das Forschungsteam Unexplored Realities der Universität St. Gallen in ihrer Studie zum Cannabiskonsum im Alltag unterstützen dürfen.\nNeben der Anpassung der Umfrage und deskriptiven statistischen Analysen habe ich in einer explorativen Clusteranalyse vorläufige Ergebnisse zu möglichen Konsumgruppen innerhalb der Konsument:innen herausarbeiten können. Diese durfte ich im Mai 2023 an einem Forschungsgespräch zur Zukunft der Cannabisforschung an der Universität St. Gallen vorstellen."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projekte",
    "section": "",
    "text": "Podcast - Besserwisserin (pausiert)\n\n\n\npersonal\n\n\npodcast\n\n\n\n\n\n\n\nLisa Götschi\n\n\n01.12.2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfragnach - Quantitative Unterstützung Studie ‘Cannabis im Alltag’\n\n\n\nacademic\n\n\nresearch\n\n\nfragnach\n\n\n\n\n\n\n\nLisa Götschi\n\n\n01.05.2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSentiment Analysis Twitter Daten zum Ukrainekrieg 2022\n\n\n\nacademic\n\n\ncode\n\n\n\n\n\n\n\nLisa Götschi\n\n\n01.12.2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBachelorarbeit und Poster\n\n\n\nacademic\n\n\nwriting\n\n\n\n\n\n\n\nLisa Götschi\n\n\n01.09.2021\n\n\n\n\n\n\n\n\nKeine Treffer"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Über mich",
    "section": "",
    "text": "Nach meinem Bachelor in Soziologie und Geografie in Zürich habe ich meinen Master in Soziologie an der Universität Bern begonnen. Sowie ein Gaststudium an der Universität Basel absolviert. Durch meine frühere Tätigkeit als Tutorin und die Arbeit als wissenschaftliche Hilfsassistentin hat sich mein Interesse an quantitativer Datenanalyse und der Wissenschaft deutlich herauskristallisiert.\nNeben quantitativen Methoden interessieren mich innerhalb der Soziologie besonders Sozialkapitaltheorien, soziale Ungleichheit und Fragestellungen, die sich (on- & offline) intersektional mit räumlichen und sozioökonomischen Aspekten befassen.\n\n\n\n   CV\n  Projekte"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "Download"
  },
  {
    "objectID": "cv.html#ausbildung",
    "href": "cv.html#ausbildung",
    "title": "CV",
    "section": "Ausbildung",
    "text": "Ausbildung\n\nMaster of Arts in Soziologie Universität Bern Sep. 2022 - heute\nMono: Soziologie\nmit Gaststudium an der Universität Basel von Sep. 2023 bis Jul. 2024\n\nBachelor of Arts in Sozialwissenschaften Universität Zürich Sep. 2018 - Jul. 2022\nMajor: Soziologie und Minor: Geographie\nAbschlussnote: 5.1\nBachelorarbeit: Subjektives Wohlbefinden – ein Graben zwischen Stadt und Land?\n\nGymnasiale Matur Gymnasium am Münsterplatz Basel Aug. 2014 - Jun. 2017\nBilinguale Matur (D/E) & International Baccalaureate (IB)\nSchwerpunkt: Englisch\nMaturarbeit: Sustainable Fashion - Comparison of two brands H&M conscious and Coop Naturaline."
  },
  {
    "objectID": "cv.html#arbeitserfahrung",
    "href": "cv.html#arbeitserfahrung",
    "title": "CV",
    "section": "Arbeitserfahrung",
    "text": "Arbeitserfahrung\n\nCo-founder fragnach Dez 2020 bis heute\nfragnach.ch Zürich\nQuantitative Methodenberatung und Projektbetreuung\nErstellen von Umfragen\nRecherche von Theorien und Studien\n\nHilfsassistentin im Projekt: “The neighborhood in the cloud” Mai 2022 bis Jul. 2024\nUniversität Bern - Institut für Soziologie\nUnterstützung in der Umsetzung des Projektes z.B. Daten- und Literaturrecherche, Aufbereiten von Daten, eigenständige quantitative Analysen.\n\nTutorin für Techniken wissenschaftlichen Arbeitens Sep. 2021 bis Feb. 2022\nUniversität Zürich\nLeiten und organisieren von Tutoraten für Studierende & Leistungsnachweiskorrekturen\n\nTutorin für empirische Sozialforschung und Umfragemethoden Sep. 2020 bis Jun. 2022\nUniversität Zürich\nLeiten und organisieren von Tutoraten für Studierende & Leistungsnachweiskorrekturen, Gruppenbetreuung"
  },
  {
    "objectID": "cv.html#weitere-erfahrung",
    "href": "cv.html#weitere-erfahrung",
    "title": "CV",
    "section": "Weitere Erfahrung",
    "text": "Weitere Erfahrung\n\n\nHilfsassistentin bei Dr. Robert Schäfer Dez. 2023\nUniversität Basel - Departement Gesellschaftswissenschaften\nKodieren von Interviews mit ATLAS.ti\n\nTeilnehmerin Fundraising Academy Jan. 2021 - Apr. 2021\nDigital Innovation Lab AG, Bern\nLernen von Strategien zum Start-up Aufbau, Investorenfindung\nErstellen von Businessplan und Marketingstrategien\n\nSoziologie-Fachverein Vorstandsmitglied Sep. 2019 - Sep. 2021\nUniversität Zürich\nVerantwortlich für Design und Druck von Postern, Flyern etc. & Eventorganisation"
  },
  {
    "objectID": "cv.html#it-kompetenzen",
    "href": "cv.html#it-kompetenzen",
    "title": "CV",
    "section": "IT Kompetenzen",
    "text": "IT Kompetenzen\n\n\n\nStatistiksoftware R & STATA\nsehr gute Kenntnisse\n\n\nMicrosoft Office (Word, Powerpoint und Excel)\nsehr gute Kenntnisse\n\n\nAdobe InDesign, Illustrator\ngute Kenntnisse\n\n\nSNAP & QGIS\nGrundkentnisse\n\n\nWordPress, CSM Magnolia\nGrundkentnisse"
  },
  {
    "objectID": "cv.html#sprachen",
    "href": "cv.html#sprachen",
    "title": "CV",
    "section": "Sprachen",
    "text": "Sprachen\n\n\n\nDeutsch\nMuttersprache\n\n\nEnglisch\nZweisprachige Matur (C1)\n\n\nFranzösisch\nGrundlagenkenntisse (A1)\n\n\nItalienisch\nGrundlagenkenntisse (A1)"
  },
  {
    "objectID": "cv.html#hobbys-und-interessen",
    "href": "cv.html#hobbys-und-interessen",
    "title": "CV",
    "section": "Hobbys und Interessen",
    "text": "Hobbys und Interessen\n\nMit Yoga und ausgiebigen Spaziergängen mit meinem Hund halte ich mich fit. Meditation und Häkeln geben mir einen Ausgleich zum Arbeitsalltag und durch das Lesen von Sachliteratur wie auch Romanen erweitere ich meinen Wissenshorizont."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Keine Treffer"
  }
]